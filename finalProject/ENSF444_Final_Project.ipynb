{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "XOOs70pZsAKF",
      "metadata": {
        "id": "XOOs70pZsAKF"
      },
      "source": [
        "<font size=\"+3\"><b>Final Project: PCA and Clustering\n",
        "</b></font>\n",
        "\n",
        "***\n",
        "* **Full Name** =\n",
        "* **UCID** =\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba7f05d0",
      "metadata": {
        "id": "ba7f05d0"
      },
      "source": [
        "<font color='Blue'>\n",
        "The purpose of this assignment is to practice using PCA and clustering techniques on a given dataset.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-zeSnLrGawW2",
      "metadata": {
        "id": "-zeSnLrGawW2"
      },
      "source": [
        "<font color='Red'>\n",
        "For this assignment, in addition to your .ipynb file, please also attach a PDF file. To generate this PDF file, you can use the print function (located under the \"File\" within Jupyter Notebook). Name this file ENGG444_Final_Project_yourUCID.pdf (this name is similar to your main .ipynb file). We will evaluate your assignment based on the two files and you need to provide both.\n",
        "</font>\n",
        "\n",
        "\n",
        "|                 **Question**                | **Point(s)** |\n",
        "|:-------------------------------------------:|:------------:|\n",
        "| **1. Principle Component   Analysis (PCA)** |              |\n",
        "|                     1.1                     |       3      |\n",
        "|                     1.2                     |       2      |\n",
        "|                     1.3                     |       2      |\n",
        "|                     1.4                     |       3      |\n",
        "|                     1.5                     |       6      |\n",
        "|                     1.6                     |       2      |\n",
        "|         **2. Pipeline and Modeling**        |              |\n",
        "|                     2.1                     |       3      |\n",
        "|                     2.2                     |       2      |\n",
        "|                     2.3                     |       2      |\n",
        "|                     2.4                     |       3      |\n",
        "|            **3. Bonus Question**            |     **2**    |\n",
        "|                    Total                    |      28      |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe7cfa4f",
      "metadata": {
        "id": "fe7cfa4f"
      },
      "source": [
        "## Data\n",
        "\n",
        "The data on [this page](https://archive.ics.uci.edu/dataset/236/seeds) pertains to a study on wheat kernels, specifically focusing on the geometrical properties of kernels from three different wheat varieties: Kama, Rosa, and Canadian. Here's a summary of the key points:\n",
        "\n",
        "- **Dataset Characteristics**: The data is multivariate and real-valued, used for classification and clustering tasks in biology.\n",
        "- **Measurement Technique**: A soft X-ray technique was employed for high-quality visualization of the internal kernel structure, which is non-destructive and cost-effective compared to other methods.\n",
        "- **Geometric Parameters**: Seven parameters were measured for each kernel: area (A), perimeter (P), compactness (C = 4*pi*A/P^2), length, width, asymmetry coefficient, and length of kernel groove.\n",
        "- **Research Purpose**: The dataset facilitates the analysis of features in X-ray images of wheat kernels and can be applied to various statistical and machine learning tasks.\n",
        "\n",
        "This dataset was collected for an experiment conducted at the Institute of Agrophysics of the Polish Academy of Sciences in Lublin and has been cited in several research papers for its application in feature analysis and classification algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uhbvQcUgaP1B",
      "metadata": {
        "id": "uhbvQcUgaP1B"
      },
      "outputs": [],
      "source": [
        "# Download the zip file using wget\n",
        "!wget -N \"https://archive.ics.uci.edu/static/public/236/seeds.zip\"\n",
        "\n",
        "# Unzip wine.data from the downloaded zip file\n",
        "!unzip -o seeds.zip seeds_dataset.txt\n",
        "\n",
        "# Remove the downloaded zip file after extraction\n",
        "!rm -r seeds.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "474481f1",
      "metadata": {
        "id": "474481f1"
      },
      "outputs": [],
      "source": [
        "# 1. area A,\n",
        "# 2. perimeter P,\n",
        "# 3. compactness C = 4*pi*A/P^2,\n",
        "# 4. length of kernel,\n",
        "# 5. width of kernel,\n",
        "# 6. asymmetry coefficient\n",
        "# 7. length of kernel groove.\n",
        "\n",
        "# https://archive.ics.uci.edu/dataset/236/seeds\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('seeds_dataset.txt', sep = '\\s+', header = None)\n",
        "data.columns = ['Area', 'Perimeter', 'Compactness',\n",
        "                'Length of Kernel', 'Width of Kernel',\n",
        "                'Asymmetry Coefficient', 'Length of Kernel Groove', 'Type']\n",
        "display(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c84e8e3",
      "metadata": {
        "id": "3c84e8e3"
      },
      "source": [
        "## 1. Principle Component Analysis (PCA)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5024bec",
      "metadata": {
        "id": "e5024bec"
      },
      "source": [
        "### 1.1 Preprocessing (3 Points)\n",
        "\n",
        "- **Split the data into X and y** (0.5 Point)\n",
        "  - Assign the features to `X` and the target variable to `y`.\n",
        "\n",
        "- **Stratified Split of X and y into Train and Test Sets** (0.5 Point)\n",
        "  - Utilize stratification to ensure representative distribution of classes while splitting.\n",
        "\n",
        "- **Plot Train and Test Proportions in a Pie Chart** (2 Points)\n",
        "  - The pie chart should include:\n",
        "    - Labels indicating 'Training Set' and 'Test Set'.\n",
        "    - A title for the chart.\n",
        "    - Proportion percentages for the Training and Test sets displayed on each slice of the pie.\n",
        "    - The number of entries within the Training and Test sets shown below the corresponding percentage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96076711",
      "metadata": {},
      "outputs": [],
      "source": [
        "# check for null values\n",
        "print(data.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b19a9948",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plotTestTrain(X_train, X_test):\n",
        "  # Calculate the number of entries in the training and test sets\n",
        "  train_count = len(X_train)\n",
        "  test_count = len(X_test)\n",
        "\n",
        "  # Calculate the percentages for the training and test sets\n",
        "  train_percent = train_count / (train_count + test_count) * 100\n",
        "  test_percent = test_count / (train_count + test_count) * 100\n",
        "\n",
        "  # Plotting the pie chart\n",
        "  fig, ax = plt.subplots()\n",
        "  wedges, _, labels = ax.pie([train_count, test_count], labels=['Training Set', 'Test Set'], autopct='%1.1f%%', startangle=90)\n",
        "\n",
        "  # Title for the chart\n",
        "  ax.set_title('Proportion of Training and Test Sets')\n",
        "\n",
        "  # Add the actual count below each percentage\n",
        "  for label, count in zip(labels, [train_count, test_count]):\n",
        "      ax.text(label.get_position()[0], label.get_position()[1]-0.2, f'{count}', ha='center', fontsize=12)\n",
        "\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15a5c76c",
      "metadata": {
        "id": "15a5c76c"
      },
      "outputs": [],
      "source": [
        "# 1.1\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "#Split the data into features and labels.\n",
        "X = data.drop(columns=['Type'])\n",
        "y = data['Type']\n",
        "#Stratifed split X and y into training and testing sets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n",
        "\n",
        "# Plot the pie chart\n",
        "plotTestTrain(X_train, X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "k64eg7HBbn48",
      "metadata": {
        "id": "k64eg7HBbn48"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **1.1** ....................."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f908b657",
      "metadata": {
        "id": "f908b657"
      },
      "source": [
        "### 1.2 Scaling the Data (2 Points)\n",
        "\n",
        "To ensure that our preprocessing pipeline optimizes the performance of our machine learning model, we need to scale the data appropriately.\n",
        "\n",
        "- **Selecting an Appropriate Scaler**:\n",
        "  - Explain your choice of scaler for the dataset. (1 Points)\n",
        "  - Justify your decision based on the characteristics of the data and the requirements of the algorithm being used. (1 Points)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9e23112",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "282cbc43",
      "metadata": {
        "id": "282cbc43"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **1.2** ....................."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cfa23f3",
      "metadata": {
        "id": "8cfa23f3"
      },
      "source": [
        "### 1.3 Model Selection and Justification (2 Points)\n",
        "\n",
        "- **Choose an Appropriate Machine Learning Model**:\n",
        "  - Identify the model that you believe is most suitable for the dataset.\n",
        "  - Provide a justification for your choice based on the dataset's characteristics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wRdBE-rFbuM8",
      "metadata": {
        "id": "wRdBE-rFbuM8"
      },
      "outputs": [],
      "source": [
        "# 1.3\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "773d0687",
      "metadata": {
        "id": "773d0687"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **1.3** ....................."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2177add4",
      "metadata": {
        "id": "2177add4"
      },
      "source": [
        "### 1.4 Hyperparameter Optimization with Grid Search (3 Points)\n",
        "\n",
        "- **Set Up the Grid Search**:\n",
        "  - Construct a pipeline that incorporates the selected scaler from part 1.2 to standardize the data.\n",
        "  - Execute a grid search within this pipeline to identify the best hyperparameter settings for your chosen model.\n",
        "  - Provide a broad and varied range of hyperparameter values to ensure a thorough search.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ae86a37",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cw6f0w0DbvaE",
      "metadata": {
        "id": "cw6f0w0DbvaE"
      },
      "outputs": [],
      "source": [
        "# 1.4\n",
        "# Construct a pipeline that incorporates the selected scaler to standardize the data.\n",
        "RFC_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "# Provide a broad and varied range of hyperparameter values to ensure a thorough search.\n",
        "RFC_param_grid = {\n",
        "    'classifier__n_estimators': [10, 50, 75, 100],\n",
        "    'classifier__max_depth': [25, 50, 75],\n",
        "    'classifier__min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Execute a grid search within this pipeline to identify the best hyperparameter settings for your chosen model.\n",
        "grid_search = GridSearchCV(RFC_pipeline, param_grid=RFC_param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41426299",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the best settings, and validate the model performance.\n",
        "print('Best hyperparameters:', grid_search.best_params_)\n",
        "print('Test set accuracy:', grid_search.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HisdcY2Zbw4M",
      "metadata": {
        "id": "HisdcY2Zbw4M"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **1.4** ....................."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KHenrooWRjzm",
      "metadata": {
        "id": "KHenrooWRjzm"
      },
      "source": [
        "### 1.5 Dimensionality Reduction and Model Optimization (6 Points)\n",
        "\n",
        "- **Dimensionality Reduction Choice** (2 Points):\n",
        "  - Choose between PCA and t-SNE for reducing the dataset to two dimensions.\n",
        "  - Justify your selection based on the characteristics of the seeds dataset.\n",
        "\n",
        "- **Implement Dimensionality Reduction** (2 Points):\n",
        "  - Apply the chosen dimensionality reduction technique to the seeds dataset.\n",
        "  - Reduce the dataset to two dimensions as required.\n",
        "\n",
        "- **Model Optimization on Reduced Data** (2 Points):\n",
        "  - Redo the grid search from part 1.4 using the two-dimensional data.\n",
        "  - Compare the model's performance with the original higher-dimensional data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ENlNZK1ub7ws",
      "metadata": {
        "id": "ENlNZK1ub7ws"
      },
      "outputs": [],
      "source": [
        "# 1.5\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Apply PCA to the Reduce the dimensionality of the data.\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(X_train)\n",
        "\n",
        "# Pipeline again, but with PCA as well.\n",
        "RFC_pipeline_pca = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('pca', pca),\n",
        "    ('classifier', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "# Perform a grid search to find best hyperparameters.\n",
        "grid_search_pca = GridSearchCV(RFC_pipeline_pca, param_grid=RFC_param_grid, cv=5)\n",
        "grid_search_pca.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab1bdc20",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the best settings, and validate the model performance.\n",
        "print('Best hyperparameters with PCA:', grid_search_pca.best_params_)\n",
        "print('Test set accuracy with PCA:', grid_search_pca.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a81cd66",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare the model's performance, display in 3 decimal places.\n",
        "print(f'Test set accuracy, Original: {grid_search.score(X_test, y_test):.03f}')\n",
        "print(f'Test set accuracy with PCA: {grid_search_pca.score(X_test, y_test):.03f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qK_pfLzVb9Kr",
      "metadata": {
        "id": "qK_pfLzVb9Kr"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **1.5** ....................."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hLqYEWqrUnzT",
      "metadata": {
        "id": "hLqYEWqrUnzT"
      },
      "source": [
        "### 1.6 Visualizing Reduced Dimensionality Data (2 Points)\n",
        "\n",
        "- **Create a 2D Scatter Plot for Training and Testing Sets**:\n",
        "  - Generate 1-row-two-column subplots for scatter plots for the two-dimensional training and testing data obtained from part 1.5.\n",
        "  - Clearly label the x-axis and y-axis for both plots.\n",
        "  - Include a legend in each plot that distinctly represents the distribution of the three classes (you can use different shapes and colors to represent different classes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OlN_Ea9TUsVo",
      "metadata": {
        "id": "OlN_Ea9TUsVo"
      },
      "outputs": [],
      "source": [
        "# 1.6\n",
        "# Create a 2D Scatter Plot for Training and Testing Sets\n",
        "# Generate 1-row-two-column subplots for scatter plots for the two-dimensional training and testing data obtained from part 1.5.\n",
        "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "for i, (X_data, y_data, title) in enumerate([(X_train, y_train, 'Training'), (X_test, y_test, 'Testing')]):\n",
        "    # Transform the data using PCA\n",
        "    X_data_pca = pca.transform(X_data)\n",
        "\n",
        "    # Plot the data\n",
        "    for label in y.unique():\n",
        "        axs[i].scatter(X_data_pca[y_data == label, 0], X_data_pca[y_data == label, 1], label=f'Type {label}')\n",
        "\n",
        "    axs[i].set_title(f'{title} Data')\n",
        "    axs[i].set_xlabel('Principal Component 1')\n",
        "    axs[i].set_ylabel('Principal Component 2')\n",
        "    axs[i].legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bf578b1",
      "metadata": {
        "id": "3bf578b1"
      },
      "source": [
        "## 2. Clustering and Visualization of the Seeds Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tSuKPKYtXOqo",
      "metadata": {
        "id": "tSuKPKYtXOqo"
      },
      "source": [
        "### 2.1 Create a Pipeline for Scaling and K-Means Clustering (3 Points)\n",
        "\n",
        "- Construct a pipeline that includes a scaler and the K-Means clustering algorithm.\n",
        "- Use the `KelbowVisualizer` with `metric='calinski_harabasz'` from Yellowbrick to determine the optimal number of clusters, `k`.\n",
        "- Explain the results of the `KelbowVisualizer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7850c256",
      "metadata": {},
      "outputs": [],
      "source": [
        "# import k-means and kelbow_visualizer\n",
        "from sklearn.cluster import KMeans\n",
        "from yellowbrick.cluster import KElbowVisualizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u12HFmYNXQvg",
      "metadata": {
        "id": "u12HFmYNXQvg"
      },
      "outputs": [],
      "source": [
        "# 2.1\n",
        "# Construct a pipeline that includes standard scaler and the K-Means clustering algorithm.\n",
        "kmeans_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('kmeans', KMeans(n_clusters=3))\n",
        "])\n",
        "\n",
        "# Use the `KelbowVisualizer` with `metric='calinski_harabasz'` from Yellowbrick to determine the optimal number of clusters, `k`.\n",
        "visualizer = KElbowVisualizer(kmeans_pipeline.named_steps['kmeans'], k=(2, 10), metric='calinski_harabasz')\n",
        "visualizer.fit(X_train)\n",
        "visualizer.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ubYW3yzIYOev",
      "metadata": {
        "id": "ubYW3yzIYOev"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **2.1** ....................."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RSI8FWbVYsQU",
      "metadata": {
        "id": "RSI8FWbVYsQU"
      },
      "source": [
        "### 2.2 Label the Data Using the Optimal Number of Clusters (2 Points)\n",
        "- Label the training data using the pipeline that includes both the scaler and K-Means with the optimal `k` found in part 2.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oghD_WIAYtn7",
      "metadata": {
        "id": "oghD_WIAYtn7"
      },
      "outputs": [],
      "source": [
        "# 2.2\n",
        "# Add necessary code here."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Wbq4Xt5uYyZH",
      "metadata": {
        "id": "Wbq4Xt5uYyZH"
      },
      "source": [
        "### 2.3 Dimensionality Reduction Using PCA (2 Points)\n",
        "- Apply PCA to reduce the dimensionality of the dataset to 2D."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6Off0lQlYz8o",
      "metadata": {
        "id": "6Off0lQlYz8o"
      },
      "outputs": [],
      "source": [
        "# 2.3\n",
        "# Add necessary code here."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "He9Ku5U_Y4Pn",
      "metadata": {
        "id": "He9Ku5U_Y4Pn"
      },
      "source": [
        "#### 2.4 Plot the 2D Data with Cluster Labels (3 Points)\n",
        "- Create a 2D scatter plot of the PCA-reduced data.\n",
        "- Color the points using the labels obtained from K-Means clustering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "POBknNOwY6Oz",
      "metadata": {
        "id": "POBknNOwY6Oz"
      },
      "outputs": [],
      "source": [
        "# 2.4\n",
        "# Add necessary code here."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4kILrSH9aDvn",
      "metadata": {
        "id": "4kILrSH9aDvn"
      },
      "source": [
        "## Bonus Question: Interpretation of Clustering Results (2 Points)\n",
        "\n",
        "- **Analyze and Interpret the Clustering Outcome**:\n",
        "  - Based on the 2D PCA plot with K-Means clustering labels from part 2.4, provide an interpretation of the clustering results.\n",
        "  - Discuss any patterns or insights observed from the plot, considering the distribution and overlap of clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nUOBtxvfcVbW",
      "metadata": {
        "id": "nUOBtxvfcVbW"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **Bonus Question** ....................."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
