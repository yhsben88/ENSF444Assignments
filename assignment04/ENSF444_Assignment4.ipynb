{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "92778525",
      "metadata": {
        "id": "92778525"
      },
      "source": [
        "<font size=\"+3\"><b>Assignment 4: Pipelines and Hyperparameter Tuning</b></font>\n",
        "\n",
        "***\n",
        "* **Full Name** =\n",
        "* **UCID** =\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce31b39a",
      "metadata": {
        "id": "ce31b39a"
      },
      "source": [
        "<font color='Blue'>\n",
        "In this assignment, you will be putting together everything you have learned so far. You will need to do all the appropriate preprocessing, test different supervised learning models, and evaluate the results. More details for each step can be found below. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "T0uItvnoRoUB",
      "metadata": {
        "id": "T0uItvnoRoUB"
      },
      "source": [
        "<font color='Red'>\n",
        "For this assignment, in addition to your .ipynb file, please also attach a PDF file. To generate this PDF file, you can use the print function (located under the \"File\" within Jupyter Notebook). Name this file ENGG444_Assignment##__yourUCID.pdf (this name is similar to your main .ipynb file). We will evaluate your assignment based on the two files and you need to provide both.\n",
        "</font>\n",
        "\n",
        "\n",
        "|         **Question**         | **Point(s)** |\n",
        "|:----------------------------:|:------------:|\n",
        "|  **1. Preprocessing Tasks**  |              |\n",
        "|              1.1             |       2      |\n",
        "|              1.2             |       2      |\n",
        "|              1.3             |       4      |\n",
        "| **2. Pipeline and Modeling** |              |\n",
        "|              2.1             |       3      |\n",
        "|              2.2             |       6      |\n",
        "|              2.3             |       5      |\n",
        "|              2.4             |       3      |\n",
        "|     **3. Bonus Question**    |     **2**    |\n",
        "|           **Total**          |    **25**    |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OpeMjIV9VLgM",
      "metadata": {
        "id": "OpeMjIV9VLgM"
      },
      "source": [
        "## **0. Dataset**\n",
        "\n",
        "This data is a subset of the **Heart Disease Dataset**, which contains information about patients with possible coronary artery disease. The data has **14 attributes** and **294 instances**. The attributes include demographic, clinical, and laboratory features, such as age, sex, chest pain type, blood pressure, cholesterol, and electrocardiogram results. The last attribute is the **diagnosis of heart disease**, which is a categorical variable with values from 0 (no presence) to 4 (high presence). The data can be used for **classification** tasks, such as predicting the presence or absence of heart disease based on the other attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6c06fcfd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: pandas in /Users/ben/Library/Python/3.9/lib/python/site-packages (2.2.0)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /Users/ben/Library/Python/3.9/lib/python/site-packages (from pandas) (1.24.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/ben/Library/Python/3.9/lib/python/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ben/Library/Python/3.9/lib/python/site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/ben/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "YiaUdCQYVWj-",
      "metadata": {
        "id": "YiaUdCQYVWj-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/hk/2btvw_ms30b3wncwb_4d923r0000gn/T/ipykernel_91630/1182941715.py:1: DeprecationWarning: \n",
            "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
            "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
            "but was not found to be installed on your system.\n",
            "If this would cause problems for you,\n",
            "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
            "        \n",
            "  import pandas as pd\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>num</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>28</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>130.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>185.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>29</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>120.0</td>\n",
              "      <td>243.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>29</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>140.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>170.0</td>\n",
              "      <td>237.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>100.0</td>\n",
              "      <td>219.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>289</th>\n",
              "      <td>52</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>160.0</td>\n",
              "      <td>331.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290</th>\n",
              "      <td>54</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>130.0</td>\n",
              "      <td>294.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>291</th>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>155.0</td>\n",
              "      <td>342.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>292</th>\n",
              "      <td>58</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>180.0</td>\n",
              "      <td>393.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>293</th>\n",
              "      <td>65</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>130.0</td>\n",
              "      <td>275.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>294 rows × 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     age  sex  cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
              "0     28    1   2     130.0  132.0  0.0      2.0    185.0    0.0      0.0   \n",
              "1     29    1   2     120.0  243.0  0.0      0.0    160.0    0.0      0.0   \n",
              "2     29    1   2     140.0    NaN  0.0      0.0    170.0    0.0      0.0   \n",
              "3     30    0   1     170.0  237.0  0.0      1.0    170.0    0.0      0.0   \n",
              "4     31    0   2     100.0  219.0  0.0      1.0    150.0    0.0      0.0   \n",
              "..   ...  ...  ..       ...    ...  ...      ...      ...    ...      ...   \n",
              "289   52    1   4     160.0  331.0  0.0      0.0     94.0    1.0      2.5   \n",
              "290   54    0   3     130.0  294.0  0.0      1.0    100.0    1.0      0.0   \n",
              "291   56    1   4     155.0  342.0  1.0      0.0    150.0    1.0      3.0   \n",
              "292   58    0   2     180.0  393.0  0.0      0.0    110.0    1.0      1.0   \n",
              "293   65    1   4     130.0  275.0  0.0      1.0    115.0    1.0      1.0   \n",
              "\n",
              "     slope  ca  thal  num  \n",
              "0      NaN NaN   NaN    0  \n",
              "1      NaN NaN   NaN    0  \n",
              "2      NaN NaN   NaN    0  \n",
              "3      NaN NaN   6.0    0  \n",
              "4      NaN NaN   NaN    0  \n",
              "..     ...  ..   ...  ...  \n",
              "289    NaN NaN   NaN    1  \n",
              "290    2.0 NaN   NaN    1  \n",
              "291    2.0 NaN   NaN    1  \n",
              "292    2.0 NaN   7.0    1  \n",
              "293    2.0 NaN   NaN    1  \n",
              "\n",
              "[294 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the data source link\n",
        "_link = 'https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data'\n",
        "\n",
        "# Read the CSV file into a Pandas DataFrame, considering '?' as missing values\n",
        "df = pd.read_csv(_link, na_values='?',\n",
        "                 names=['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs',\n",
        "                        'restecg', 'thalach', 'exang', 'oldpeak', 'slope',\n",
        "                        'ca', 'thal', 'num'])\n",
        "\n",
        "# Display the DataFrame\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mlcrJpGLWBOH",
      "metadata": {
        "id": "mlcrJpGLWBOH"
      },
      "source": [
        "# **1. Preprocessing Tasks**\n",
        "\n",
        "- **1.1** Find out which columns have more than 60% of their values missing and drop them from the data frame. Explain why this is a reasonable way to handle these columns. **(2 Points)**\n",
        "\n",
        "- **1.2** For the remaining columns that have some missing values, choose an appropriate imputation method to fill them in. You can use the `SimpleImputer` class from `sklearn.impute` or any other method you prefer. Explain why you chose this method and how it affects the data. **(2 Points)**\n",
        "\n",
        "- **1.3** Assign the `num` column to the variable `y` and the rest of the columns to the variable `X`. The `num` column indicates the presence or absence of heart disease based on the angiographic disease status of the patients. Create a `ColumnTransformer` object that applies different preprocessing steps to different subsets of features. Use `StandardScaler` for the numerical features, `OneHotEncoder` for the categorical features, and `passthrough` for the binary features. List the names of the features that belong to each group and explain why they need different transformations. You will use this `ColumnTransformer` in a pipeline in the next question. **(4 Points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yyRJQ25hXHNF",
      "metadata": {
        "id": "yyRJQ25hXHNF"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **1.1** \n",
        "I found the percentage of NaN values per column, then dropped the columns with 60% or higher NaN occurences using the pandas built in drop function, using '.index' to identify the columns to be dropped, as a list for the parameter 'columns'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "NzUkBHBfYBzF",
      "metadata": {
        "id": "NzUkBHBfYBzF"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['slope', 'ca', 'thal'], dtype='object')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 1.1\n",
        "isNaPercentage = df.isna().mean()\n",
        "columnsToDrop = isNaPercentage[isNaPercentage > 0.6].index\n",
        "columnsToDrop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6a629339",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>num</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>28</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>130.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>185.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>29</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>120.0</td>\n",
              "      <td>243.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>29</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>140.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>170.0</td>\n",
              "      <td>237.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>100.0</td>\n",
              "      <td>219.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>289</th>\n",
              "      <td>52</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>160.0</td>\n",
              "      <td>331.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290</th>\n",
              "      <td>54</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>130.0</td>\n",
              "      <td>294.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>291</th>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>155.0</td>\n",
              "      <td>342.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>292</th>\n",
              "      <td>58</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>180.0</td>\n",
              "      <td>393.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>293</th>\n",
              "      <td>65</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>130.0</td>\n",
              "      <td>275.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>294 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     age  sex  cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  num\n",
              "0     28    1   2     130.0  132.0  0.0      2.0    185.0    0.0      0.0    0\n",
              "1     29    1   2     120.0  243.0  0.0      0.0    160.0    0.0      0.0    0\n",
              "2     29    1   2     140.0    NaN  0.0      0.0    170.0    0.0      0.0    0\n",
              "3     30    0   1     170.0  237.0  0.0      1.0    170.0    0.0      0.0    0\n",
              "4     31    0   2     100.0  219.0  0.0      1.0    150.0    0.0      0.0    0\n",
              "..   ...  ...  ..       ...    ...  ...      ...      ...    ...      ...  ...\n",
              "289   52    1   4     160.0  331.0  0.0      0.0     94.0    1.0      2.5    1\n",
              "290   54    0   3     130.0  294.0  0.0      1.0    100.0    1.0      0.0    1\n",
              "291   56    1   4     155.0  342.0  1.0      0.0    150.0    1.0      3.0    1\n",
              "292   58    0   2     180.0  393.0  0.0      0.0    110.0    1.0      1.0    1\n",
              "293   65    1   4     130.0  275.0  0.0      1.0    115.0    1.0      1.0    1\n",
              "\n",
              "[294 rows x 11 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.drop(columns = columnsToDrop, inplace = True) # default: inplace = False\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xkk6IDQRXgJM",
      "metadata": {
        "id": "xkk6IDQRXgJM"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **1.2** Through interpretation of which features are numerical and categorical respectively, impute numerical features with mean strategy and categorical features with most frequent strategy; Categorical features should not have decimals as the integer assigned to it represents ordinal encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e4f33deb",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "age          0\n",
              "sex          0\n",
              "cp           0\n",
              "trestbps     1\n",
              "chol        23\n",
              "fbs          8\n",
              "restecg      1\n",
              "thalach      1\n",
              "exang        1\n",
              "oldpeak      0\n",
              "num          0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "t7Hw48YkZcCb",
      "metadata": {
        "id": "t7Hw48YkZcCb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "age         0\n",
              "sex         0\n",
              "cp          0\n",
              "trestbps    0\n",
              "chol        0\n",
              "fbs         0\n",
              "restecg     0\n",
              "thalach     0\n",
              "exang       0\n",
              "oldpeak     0\n",
              "num         0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 1.2\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "numericalFeatures = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\"]\n",
        "categoricalFeatures = [\"sex\", \"cp\", \"fbs\", \"restecg\", \"exang\"]\n",
        "\n",
        "numImputer = SimpleImputer(strategy = \"mean\")\n",
        "catImputer = SimpleImputer(strategy = \"most_frequent\")\n",
        "\n",
        "df[numericalFeatures] = numImputer.fit_transform(df[numericalFeatures])\n",
        "df[categoricalFeatures] = catImputer.fit_transform(df[categoricalFeatures])\n",
        "\n",
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TS8GSVmmXoOg",
      "metadata": {
        "id": "TS8GSVmmXoOg"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **1.3** \n",
        "\n",
        "    - numerical features: \"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\"\n",
        "        - Standard Scaler is normalization meant for numerical data.\n",
        "    - categorical features: \"cp\", \"restecg\"\n",
        "        - One Hot Encoding expresses categorical data as binary.\n",
        "    - binary features: \"sex\", \"fbs\", \"exang\"\n",
        "        - Scaling not neccessary for binary data as it is neither continuous nor categorical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "pCxLaTmQXYC7",
      "metadata": {
        "id": "pCxLaTmQXYC7"
      },
      "outputs": [],
      "source": [
        "# 1.3\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "numericalFeatures = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\"]\n",
        "categoricalFeatures = [\"cp\", \"restecg\"]\n",
        "binaryFeatures = [\"sex\", \"fbs\", \"exang\"]\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers = [\n",
        "        (\"num\", StandardScaler(), numericalFeatures),\n",
        "        (\"cat\", OneHotEncoder(), categoricalFeatures),\n",
        "        (\"bin\", \"passthrough\", binaryFeatures)\n",
        "    ],\n",
        "    remainder = \"drop\"\n",
        ")\n",
        "\n",
        "X = df.drop(columns = \"num\")\n",
        "y = df[\"num\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a245d00",
      "metadata": {
        "id": "2a245d00"
      },
      "source": [
        "# **2. Pipeline and Modeling**\n",
        "\n",
        "- **2.1** Create **three** `Pipeline` objects that take the column transformer from the previous question as the first step and add one or more models as the subsequent steps. You can use any models from `sklearn` or other libraries that are suitable for binary classification. For each pipeline, explain **why** you selected the model(s) and what are their **strengths and weaknesses** for this data set. **(3 Points)**\n",
        "\n",
        "- **2.2** Use `GridSearchCV` to perform a grid search over the hyperparameters of each pipeline and find the best combination that maximizes the cross-validation score. Report the best parameters and the best score for each pipeline. Then, update the hyperparameters of each pipeline using the best parameters from the grid search. **(6 Points)**\n",
        "\n",
        "- **2.3** Form a stacking classifier that uses the three pipelines from the previous question as the base estimators and a meta-model as the `final_estimator`. You can choose any model for the meta-model that is suitable for binary classification. Explain **why** you chose the meta-model and how it combines the predictions of the base estimators. Then, use `StratifiedKFold` to perform a cross-validation on the stacking classifier and present the accuracy scores and F1 scores for each fold. Report the mean and the standard deviation of each score in the format of `mean ± std`. For example, `0.85 ± 0.05`. **(5 Points)**\n",
        "\n",
        "- **2.4**: Interpret the final results of the stacking classifier and compare its performance with the individual models. Explain how stacking classifier has improved or deteriorated the prediction accuracy and F1 score, and what are the possible reasons for that. **(3 Points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GSpSIu-BY1Kn",
      "metadata": {
        "id": "GSpSIu-BY1Kn"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **2.1** \n",
        "    - logistic regression: \n",
        "        - For simplicity.\n",
        "        - Strengths: Simple to implement, easy to interpret.\n",
        "        - Weaknesses: Sensitivity to outliers, poor handling of non-linear relationships.\n",
        "    - gradient boosting:\n",
        "        - More complexity when fitting.\n",
        "        - Strengths: Works well with mixture of binary and continuous features, good scaling.\n",
        "        - Weaknesses: Sensitive to hyperparameters. Poor performance with high-dimensional sparse data.\n",
        "    - SVC:\n",
        "        - Performs well on a large variety of datasets.\n",
        "        - Strengths: Works well in both low and high dimensional data.\n",
        "        - Weaknesses: Don't scale with number of samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2fea4d90",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2e5188de",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "qYMtXgFtOBMT",
      "metadata": {
        "id": "qYMtXgFtOBMT"
      },
      "outputs": [],
      "source": [
        "# 2.1\n",
        "lrModel = LogisticRegression()\n",
        "gbModel = GradientBoostingClassifier()\n",
        "svcModel = SVC()\n",
        "\n",
        "# lrPipe = make_pipeline(preprocessor, lrModel)\n",
        "# gbPipe = make_pipeline(preprocessor, gbModel)\n",
        "# svcPipe = make_pipeline(preprocessor, svcModel)\n",
        "\n",
        "lrPipe = Pipeline([\n",
        "    (\"preprocessing\", preprocessor),\n",
        "    (\"classifier\", lrModel)\n",
        "    ])\n",
        "\n",
        "gbPipe = Pipeline([\n",
        "    (\"preprocessing\", preprocessor),\n",
        "    (\"classifier\", gbModel)\n",
        "    ])\n",
        "\n",
        "svcPipe = Pipeline([\n",
        "    (\"preprocessing\", preprocessor),\n",
        "    (\"classifier\", svcModel)\n",
        "    ])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NPSo4pBVe1GR",
      "metadata": {
        "id": "NPSo4pBVe1GR"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **2.2** \n",
        "- The best hyper parameters for:\n",
        "    - LogisticRegression:\n",
        "        - C = 1\n",
        "    - GradientBoostingClassifier:\n",
        "        - learning rate = 1\n",
        "        - n_estimators = 150\n",
        "    - SVC:\n",
        "        - C = 0.01\n",
        "        - gamma = 0.001 \n",
        "        - kernel = linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "sNXYl9WFe3vA",
      "metadata": {
        "id": "sNXYl9WFe3vA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model:\tLogisticRegression(C=1)\n",
            "\tBest parameters: {'classifier__C': 1}\n",
            "\tBest score: 0.8170212765957446\n",
            "\n",
            "\n",
            "Model:\tGradientBoostingClassifier(learning_rate=1, n_estimators=150)\n",
            "\tBest parameters: {'classifier__learning_rate': 1, 'classifier__n_estimators': 150}\n",
            "\tBest score: 0.7914893617021276\n",
            "\n",
            "\n",
            "Model:\tSVC(C=0.01, gamma=0.001, kernel='linear')\n",
            "\tBest parameters: {'classifier__C': 0.01, 'classifier__gamma': 0.001, 'classifier__kernel': 'linear'}\n",
            "\tBest score: 0.8382978723404255\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 2.2\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "lrParamGrid = {'classifier__C': [0.001, 0.01, 0.1, 1, 10]}\n",
        "gbParamGrid = {'classifier__n_estimators': [50, 100, 150], 'classifier__learning_rate': [0.01, 0.1, 1]}\n",
        "svcParamGrid = {'classifier__C': [0.001, 0.01, 0.1, 1, 10], 'classifier__gamma': [0.001, 0.01, 0.1, 1], 'classifier__kernel': ['linear', 'rbf']}\n",
        "\n",
        "paramGrids = [lrParamGrid, gbParamGrid, svcParamGrid]\n",
        "pipes = [lrPipe, gbPipe, svcPipe]\n",
        "models = [lrModel, gbModel, svcModel]\n",
        "best_params = []\n",
        "\n",
        "for paramGrid, pipe, model in zip(paramGrids, pipes, models):\n",
        "    gridSearch = GridSearchCV(pipe, paramGrid, cv = 5)\n",
        "    gridSearch.fit(X_train, y_train)\n",
        "    print(f\"Model:\\t{model}\")\n",
        "    print(f\"\\tBest parameters: {gridSearch.best_params_}\")\n",
        "    print(f\"\\tBest score: {gridSearch.best_score_}\")\n",
        "    best_params.append(gridSearch.best_params_)\n",
        "    print(\"\\n\")\n",
        "\n",
        "lrParams = best_params[0]\n",
        "gbParams = best_params[1]\n",
        "svcParams = best_params[2]\n",
        "\n",
        "lrModel = LogisticRegression(C = lrParams[\"classifier__C\"])\n",
        "gbModel = GradientBoostingClassifier(n_estimators = gbParams[\"classifier__n_estimators\"], learning_rate = gbParams[\"classifier__learning_rate\"])\n",
        "svcModel = SVC(C = svcParams[\"classifier__C\"], gamma = svcParams[\"classifier__gamma\"], kernel = svcParams[\"classifier__kernel\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ygOeNB-PamnU",
      "metadata": {
        "id": "ygOeNB-PamnU"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **2.3** I chose Logistic Regression for the final estimator meta-model as it is simple to implement and interpret. Predictions of the base estimators are treated as features of the final model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UvhsbjmYP2G_",
      "metadata": {
        "id": "UvhsbjmYP2G_"
      },
      "outputs": [],
      "source": [
        "# 2.3\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "classifierStack =  StackingClassifier(\n",
        "    estimators = [\n",
        "        ('lr', lrModel),\n",
        "        ('gb', gbModel),\n",
        "        ('svc', svcModel)\n",
        "    ],\n",
        "    final_estimator = LogisticRegression()\n",
        ")\n",
        "\n",
        "skf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 0)\n",
        "\n",
        "trainAccuracyList = []\n",
        "trainF1List = []\n",
        "\n",
        "testAccuracyList = []\n",
        "testF1List = []\n",
        "\n",
        "for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):\n",
        "    XTrainFold, XTestFold = X.iloc[train_idx], X.iloc[test_idx]\n",
        "    yTrainFold, yTestFold = y.iloc[train_idx], y.iloc[test_idx]\n",
        "\n",
        "    classifierStack.fit(XTrainFold, yTrainFold)\n",
        "\n",
        "    yTrainPred = classifierStack.predict(XTrainFold)\n",
        "    yTestPred = classifierStack.predict(XTestFold)\n",
        "\n",
        "    trainAccuracy = accuracy_score(yTrainFold, yTrainPred)\n",
        "    testAccuracy = accuracy_score(yTestFold, yTestPred)\n",
        "    trainF1 = f1_score(yTrainFold, yTrainPred, average = \"weighted\")\n",
        "    testF1 = f1_score(yTestFold, yTestPred, average = \"weighted\")\n",
        "\n",
        "    trainAccuracyList.append(trainAccuracy)\n",
        "    trainF1List.append(trainF1)\n",
        "    testAccuracyList.append(testAccuracy)\n",
        "    testF1List.append(testF1)\n",
        "\n",
        "trainaccuracyMean = np.mean(trainAccuracyList)\n",
        "trainaccuracyStd = np.std(trainAccuracyList)\n",
        "trainf1Mean = np.mean(trainF1List)\n",
        "trainf1Std = np.std(trainF1List)\n",
        "\n",
        "testAccuracyMean = np.mean(testAccuracyList)\n",
        "testAccuracyStd = np.std(testAccuracyList)\n",
        "testF1Mean = np.mean(testF1List)\n",
        "testF1Std = np.std(testF1List)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "90ef48a6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy: 0.8461 +/- 0.0120\n",
            "Test Accuracy: 0.8366 +/- 0.0301\n",
            "Train F1: 0.8429 +/- 0.0121\n",
            "Test F1: 0.8324 +/- 0.0312\n"
          ]
        }
      ],
      "source": [
        "print(f\"Train Accuracy: {trainaccuracyMean:.4f} +/- {trainaccuracyStd:.4f}\")\n",
        "print(f\"Test Accuracy: {testAccuracyMean:.4f} +/- {testAccuracyStd:.4f}\")\n",
        "print(f\"Train F1: {trainf1Mean:.4f} +/- {trainf1Std:.4f}\")\n",
        "print(f\"Test F1: {testF1Mean:.4f} +/- {testF1Std:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A-TN9hr3b77-",
      "metadata": {
        "id": "A-TN9hr3b77-"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **2.4** \n",
        "    - Stacking classifier results:\n",
        "        - accuracy = 0.8366\n",
        "        - f1 = 0.8324\n",
        "\n",
        "    - Logistic Regression:\n",
        "        - accuracy = 0.8644\n",
        "        - f1 = 0.8637\n",
        "        - Logistic regression shows a flat out improvement throughout, demonstrating that the dataset is likely linearly relational.\n",
        "    \n",
        "    - Gradient Boosting: \n",
        "        - accuracy = 0.8475\n",
        "        - f1 = 0.8461\n",
        "        - Gradient boosting has shown better validation results. However, this model is undesirable as it vastly overfits the training data, with a score of 1.0 for both training accuracy and training f1 score.\n",
        "\n",
        "    - SVC:\n",
        "        - accuracy = 0.8136\n",
        "        - f1 = 0.8052\n",
        "        - SVC has worse validation scores than the stacking classifier. SVC is sensitive to data preprocessing and hyperparameter settings, so there could have been other hyperparameter settings that could result in a better model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "395c9dc9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression:\n",
            "  train accuracy: 0.8511\n",
            "  test accuracy: 0.8644\n",
            "  train f1: 0.8461\n",
            "  test f1: 0.8637\n",
            "\n",
            "Gradient Boosting:\n",
            "  train accuracy: 1.0000\n",
            "  test accuracy: 0.8475\n",
            "  train f1: 1.0000\n",
            "  test f1: 0.8461\n",
            "\n",
            "SVC:\n",
            "  train accuracy: 0.8340\n",
            "  test accuracy: 0.8136\n",
            "  train f1: 0.8249\n",
            "  test f1: 0.8052\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ben/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "models = [lrModel, gbModel, svcModel]\n",
        "modelNames = [\"Logistic Regression\", \"Gradient Boosting\", \"SVC\"]\n",
        "\n",
        "for model, name in zip(models, modelNames):\n",
        "    model.fit(X_train, y_train)\n",
        "    yTrainPred = model.predict(X_train)\n",
        "    yTestpred = model.predict(X_test)\n",
        "\n",
        "    trainAccuracy = accuracy_score(y_train, yTrainPred)\n",
        "    testAccuracy = accuracy_score(y_test, yTestpred)\n",
        "    trainF1 = f1_score(y_train, yTrainPred, average = \"weighted\")\n",
        "    testF1 = f1_score(y_test, yTestpred, average = \"weighted\")\n",
        "\n",
        "    print(f\"{name}:\")\n",
        "    print(f\"  train accuracy: {trainAccuracy:.4f}\")\n",
        "    print(f\"  test accuracy: {testAccuracy:.4f}\")\n",
        "    print(f\"  train f1: {trainF1:.4f}\")\n",
        "    print(f\"  test f1: {testF1:.4f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RPa-v8Xxc7aU",
      "metadata": {
        "id": "RPa-v8Xxc7aU"
      },
      "source": [
        "**Bonus Question**: The stacking classifier has achieved a high accuracy and F1 score, but there may be still room for improvement. Suggest **two** possible ways to improve the modeling using the stacking classifier, and explain **how** and **why** they could improve the performance. **(2 points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IrSooo0DfC-V",
      "metadata": {
        "id": "IrSooo0DfC-V"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0aeb8ef7",
      "metadata": {},
      "source": [
        "1. Use a different meta-model. Logistic Regression is a good choice due to implementation simplicity, but there can be other models that result in better accuracy.\n",
        "2. More careful tuning of hyperparameters of the base and meta-models can increase performance of the stacking classifier."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
